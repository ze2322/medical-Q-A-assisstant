# ğŸ¥ Medical Q&A Assistant  
A fine-tuned **LLaMA 3.1 8B** model for answering medical questions, deployed with **Streamlit**.

---

## ğŸš€ Overview  
This project provides an AI assistant capable of answering medical questions using a **LoRA fine-tuned model** trained on the **MedQuAD** dataset.  
The model is optimized for efficient inference using **4-bit quantization**.

---

## âš ï¸ Disclaimer  
This application is **for educational purposes only**.  
It is **not** intended to provide medical advice, diagnosis, or treatment.  
Always consult a qualified healthcare professional for medical concerns.

---

## ğŸ“‚ Project Structure  

medical-qa-assistant/
â”œâ”€â”€ medical_assistant_lora/
â”‚ â”œâ”€â”€ adapter_config.json
â”‚ â”œâ”€â”€ adapter_model.safetensors
â”‚ â”œâ”€â”€ tokenizer.json
â”‚ â”œâ”€â”€ tokenizer.model
â”‚ â”œâ”€â”€ tokenizer_config.json
â”‚ â””â”€â”€ special_tokens_map.json
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“Œ Requirements

Python 3.9+.
CUDA-compatible GPU (8GB+ VRAM recommended).
~10GB disk space for model + adapters.
