## ğŸ¥ Medical Q&A Assistant

A fine-tuned LLaMA 3.1 8B model for answering medical questions, deployed with Streamlit.

## ğŸš€ Overview

This project provides an AI assistant capable of answering medical questions using a LoRA fine-tuned model trained on the MedQuAD dataset.
The model is optimized for efficient inference using 4-bit quantization.

## âš ï¸ Disclaimer

This application is for educational purposes only.
It is not intended to provide medical advice, diagnosis, or treatment.
Always consult a qualified healthcare professional for medical concerns.

## ğŸ“‚ Project Structure
medical-qa-assistant/
â”œâ”€â”€ medical_assistant_lora/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ special_tokens_map.json
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ğŸ“Œ Requirements

Python 3.9+

CUDA-compatible GPU (8GB+ VRAM recommended)

~10GB disk space for model + adapters

## ğŸ›  Installation

# 1ï¸âƒ£ Clone the repository
git clone https://github.com/yourusername/medical-qa-assistant.git
cd medical-qa-assistant

# 2ï¸âƒ£ Create a virtual environment
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ Usage

Run the Streamlit app:

streamlit run app.py


The app will open at:
http://localhost:8501

ğŸ§  Model Details

Base Model: Meta LLaMA 3.1 8B

Fine-tuning Method: LoRA

Training Dataset: MedQuAD

Quantization: 4-bit

ğŸ”§ LoRA Configuration

Rank (r): 8

Alpha: 8

Target Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

ğŸ“„ License

This project is for educational purposes only.

ğŸ™ Acknowledgments

Unsloth â€“ efficient LoRA fine-tuning

MedQuAD dataset

Meta LLaMA â€“ base model
